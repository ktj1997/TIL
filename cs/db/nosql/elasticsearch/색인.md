# 색인 (indexing)
- 데이터를 검색 할 수 있는 형식으로 바꾸기 위해서 문서를 분석하고 저장하는 과정이다.
- 원본 문서를 Token으로 쪼개어, 검색 할 수 있는 형태로 변환한다.
  - 문서 저장
  - Index 생성
  - Mapping 확인
  - InvertedIndex 생성
    - Inverted Index를 생성하는 것이 가장 무거운 작업이다.
- 색인의 성능을 높이기 위해서는 Cluster의 이점을 충분히 누리고있는지 확인해야 한다.
- 적절한 수의 Shard를 설정해야 한다.
  - 적절한 PrimaryShard를 사용하여, 색인의 성능을 높여야한다.
  - Shard의 개수에 따라서, 고르게 분배되지않고 Shard마다 용량의 불균형이 발생 할 수 있다.

## 색인 과정

<img width="1466" alt="색인과정" src="https://user-images.githubusercontent.com/57896918/213202126-247135ff-4f68-4ee4-b112-c750623f2ddb.png">


## 역 인덱스 (Inverted Index)
- 키워드에 해당하는 문서를 역으로 매핑하는 과정
- 검색은 문서에 해당하는 키워드를 찾는 행위이기 때문에, 키워드에 해당하는 문서들을 가지고 있는 것이 훨씬 빠르다.
- **ElasticSearch는 Row가 늘어나는 것이 아니라, 해당하는 문서가 배열에 추가 되는 것이기 떄문에, 속도가 크게 저하되지 않는다.**
<img width="820" alt="스크린샷 2023-01-18 오후 11 07 59" src="https://user-images.githubusercontent.com/57896918/213202294-32530a91-d289-4020-8fa1-de150ca50523.png">


## 텍스트 분석 (Text Analyze)
- 데이터에서 검색어 Token을 뽑아내는 역할을 한다.
  - 위 사진의 The, brown과 같은 Term을 의미한다.
- Analyzer가 이 역할을 수행한다.
- 아래의 3가지를 통칭해서 Analyzer라고 부른다.
  - Character Filter
  - Tokenizer
  - Token Filter

### [1] Character Filter
- Text Analyze 과정 중 가장 먼저 수행 된다.
- **Token으로 분리되기 전에 전체 데이터에 수행 하는 전처리 과정이다.**

#### <1> HTML Strip
- HTML 태그들을 모두 제거해준다.
  - 태그 뿐만 아니라, HTML 특정 문법도 제거해준다.

#### <2> Mapping
- **특정 단어를 Mapping Character Filter에 등록된 단어로 치환한다.**
- 검색의 불용어(stopwords) 처리에 따른 변환을 목적으로 자주 사용된다.
  - ex) c++ --> +가 불용어로 변경이되어, C로 검색해도 나오게된다.    
        이 때, cpp와 같은 걸로 치환하는데에 사용된다.

#### <3> Pattern Replace
- **정규식을 이용하여, 좀더 세밀하게 단어를 치환한다.**
```shell
PUT camel
{
  "settings": {
      "char_filter": {
        "camel_filter": {
          "type": "pattern_replace",
          "pattern": "(?<=\\p{Lower})(?=\\p{Upper})",
          "replacement": " "
        }
      }
    }
}
```

## [2] Tokenizer
- 문서를 Token으로 나누는 역할을 한다.
- **한개의 Tokenizer만 사용 가능하다.**
- 여러가지 Tokenizer들이 존재하며, Tokenizer가 성능에 큰 영향을 미친다.

### <1> Standard
- 공백으로 토큰을 구분한다.
- 구분한 토큰에 포함된 특수문자를 제거한다.
  - 문자 사이에 끼어있는 특수문자는 제거하지 못한다.
- ElasticSearch의 Default Tokenizer이다.
```shell
# Request
GET _analyze
{
  "tokenizer": "standard",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}

# Response
[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog's, bone ]
```
1. 단어 문자는 특정 문장 부호 및 기호뿐만 아니라 모든 문자 또는 숫자
2. 단어는 공백과 탭 및 줄 바꿈과 같은 특정 문장 부호로 구분
3. 메일 주소의 "@" 기호와 같은 특정 기호로 단어를 구분
4. 단어는 문자, 숫자 및 특정 문장 부호를 포함하여 여러 문자로 구성될 수 있다.
5. 일부 유니코드 문자는 이모지 또는 중국어 문자와 같은 자체 단어로 처리된다.
6. 토크나이저는 '-', '/' 등과 같은 숫자와 특수 문자를 인식하고 구분
7. 토크나이저는 'ing', 'ed' 등과 같은 접두사 또는 접미사가 있는 단어를 인식하는 경우도 처리한다.
8. 토크나이저는 단어를 어근 형태로 줄이는 형태소 분석 프로세스도 처리한다.



### <2> Letter

### <3> WhiteSpace